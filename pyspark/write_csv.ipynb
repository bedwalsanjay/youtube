{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4384707f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"writing csv\").master(\"local[4]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef26807c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_1=r'C:\\Users\\Sanjay Bedwal\\Desktop\\practice\\spark\\files\\csv\\input\\order1.csv'\n",
    "input_file_2=r'C:\\Users\\Sanjay Bedwal\\Desktop\\practice\\spark\\files\\csv\\input\\order2.csv'\n",
    "output_path=r'C:\\Users\\Sanjay Bedwal\\Desktop\\practice\\spark\\files\\csv\\output'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2a2f0c",
   "metadata": {},
   "source": [
    "# delimiter\n",
    "## all files who have a consistent delimiter qualifies for an csv . eg tab delimited , pipe delimeted and so on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "812f4a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n",
      "|order_id|   order_status|\n",
      "+--------+---------------+\n",
      "|       1|         CLOSED|\n",
      "|       2|PENDING_PAYMENT|\n",
      "|       3|       COMPLETE|\n",
      "|       4|         CLOSED|\n",
      "|       5|       COMPLETE|\n",
      "+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#reading first file\n",
    "csv_df1=spark.read.options(header=True,inferSchema=True,delimiter=',').csv(input_file_1)\n",
    "csv_df1=csv_df1.select('order_id','order_status')\n",
    "csv_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c3ca8c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing the output of first file\n",
    "csv_df1.write.csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "89e29c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n",
      "|order_id|   order_status|\n",
      "+--------+---------------+\n",
      "|       6|       COMPLETE|\n",
      "|       7|       COMPLETE|\n",
      "|       8|     PROCESSING|\n",
      "|       9|PENDING_PAYMENT|\n",
      "|      10|PENDING_PAYMENT|\n",
      "+--------+---------------+\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_ALREADY_EXISTS] Path file:/C:/Users/Sanjay Bedwal/Desktop/practice/spark/files/csv/output already exists. Set mode as \"overwrite\" to overwrite the existing path.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-99853488d24e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcsv_df2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcsv_df2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'order_id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'order_status'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcsv_df2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mcsv_df2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mF:\\big-data\\spark-3.0.0\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1797\u001b[0m             \u001b[0mlineSep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlineSep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1798\u001b[0m         )\n\u001b[1;32m-> 1799\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1800\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1801\u001b[0m     def orc(\n",
      "\u001b[1;32mF:\\big-data\\spark-3.0.0\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\big-data\\spark-3.0.0\\python\\pyspark\\errors\\exceptions\\captured.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    173\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [PATH_ALREADY_EXISTS] Path file:/C:/Users/Sanjay Bedwal/Desktop/practice/spark/files/csv/output already exists. Set mode as \"overwrite\" to overwrite the existing path."
     ]
    }
   ],
   "source": [
    "# reading 2nd file and writing its optput to the same location in append mode\n",
    "csv_df2=spark.read.options(header=True,inferSchema=True).csv(input_file_2)\n",
    "csv_df2=csv_df2.select('order_id','order_status')\n",
    "csv_df2.show()\n",
    "csv_df2.write.csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "329010a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing the output of first file in append mode without header\n",
    "csv_df2.write.mode('append').csv(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98590dc9",
   "metadata": {},
   "source": [
    "## no headers in output :( \n",
    "## delete the output folder and continue for rest of the demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e5b64418",
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing the output of first file in append mode\n",
    "csv_df1.write.mode('append').options(header=True).csv(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2d010368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n",
      "|order_id|   order_status|\n",
      "+--------+---------------+\n",
      "|       6|       COMPLETE|\n",
      "|       7|       COMPLETE|\n",
      "|       8|     PROCESSING|\n",
      "|       9|PENDING_PAYMENT|\n",
      "|      10|PENDING_PAYMENT|\n",
      "+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reading 2nd file and writing its optput to the same location in append mode\n",
    "csv_df2=spark.read.options(header=True,inferSchema=True).csv(input_file_2)\n",
    "csv_df2=csv_df2.select('order_id','order_status')\n",
    "csv_df2.show()\n",
    "csv_df2.write.mode('append').options(header=True,delimiter=',').csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bef12849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n",
      "|order_id|   order_status|\n",
      "+--------+---------------+\n",
      "|       6|       COMPLETE|\n",
      "|       7|       COMPLETE|\n",
      "|       8|     PROCESSING|\n",
      "|       9|PENDING_PAYMENT|\n",
      "|      10|PENDING_PAYMENT|\n",
      "|       1|         CLOSED|\n",
      "|       2|PENDING_PAYMENT|\n",
      "|       3|       COMPLETE|\n",
      "|       4|         CLOSED|\n",
      "|       5|       COMPLETE|\n",
      "+--------+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading the output file to check the content of the directory\n",
    "df=spark.read.options(header=True).csv(output_path)\n",
    "df.show()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7d4c85",
   "metadata": {},
   "source": [
    "# overrite mode and delimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0911696a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n",
      "|order_id|   order_status|\n",
      "+--------+---------------+\n",
      "|       6|       COMPLETE|\n",
      "|       7|       COMPLETE|\n",
      "|       8|     PROCESSING|\n",
      "|       9|PENDING_PAYMENT|\n",
      "|      10|PENDING_PAYMENT|\n",
      "+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reading 2nd file and writing its optput to the same location in overwrite mode\n",
    "csv_df2=spark.read.options(header=True,inferSchema=True).csv(input_file_2)\n",
    "csv_df2=csv_df2.select('order_id','order_status')\n",
    "csv_df2.show()\n",
    "csv_df2.write.mode('overwrite').options(header=True,delimiter='|').csv(output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
